{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a5d0e4-ab1a-418b-809d-f558f7f7322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEEDS LATEST WSD!\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "### NNUNET WSI INFERENCE WITH HALF OVERLAP\n",
    "#################################################################\n",
    "# nnUNet by default does half overlap if the given patch is bigger than the model's patch size.\n",
    "# This means that on the borders there is no or 1x overlap (1 or 2 predictions),\n",
    "# while in the inside there are 4 predictions for each pixel.\n",
    "\n",
    "# In this version of nnUNet WSI inference, we crop this border off the sampled patch, and only write the inner part\n",
    "# This approach becomes more efficient if bigger patches are sampled, because this increases the inner/outer ratio\n",
    "# To prevent inference on large empty patches we add a check if we can remove rows and columns,\n",
    "# while preserving the half overlap of nnUNet's sliding window approach\n",
    "\n",
    "# In this file we will also auto-configure the config and files yamls ;)\n",
    "#################################################################\n",
    "\n",
    "print('NEEDS LATEST WSD!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545ab3bd-fae3-49a9-b7ee-b0edef4a99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "### Imports\n",
    "#################################################################\n",
    "from wholeslidedata.iterators import create_batch_iterator\n",
    "from wholeslidedata.interoperability.asap.imagewriter import WholeSlideMaskWriter\n",
    "# from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "# from nnunet.training.model_restore import load_model_and_checkpoint_files\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from wholeslidedata.samplers.utils import crop_data\n",
    "import yaml\n",
    "from wholeslidedata.image.wholeslideimage import WholeSlideImage\n",
    "import time\n",
    "\n",
    "from nnunetv2.utilities.file_path_utilities import load_json\n",
    "from nnunetv2.training.nnUNetTrainer.variants.pathology.nnUNetTrainer_custom_dataloader_test import nnUNetTrainer_custom_dataloader_test\n",
    "from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b2fe58-f488-4ff1-871d-4f67e9b9c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "### Functions and utilities\n",
    "#################################################################\n",
    "current_os = \"w\" if os.name == \"nt\" else \"l\"\n",
    "other_os = \"l\" if current_os == \"w\" else \"w\"\n",
    "\n",
    "\n",
    "def convert_path(path, to=current_os):\n",
    "    \"\"\"\n",
    "    This function converts paths to the format of the desired platform.\n",
    "    By default it changes paths to the platform you are cyurerntly using.\n",
    "    This way you do not need to change your paths if you are executing your code on a different platform.\n",
    "    It may however be that you mounted the drives differently.\n",
    "    In that case you may need to change that in the code below.\n",
    "    \"\"\"\n",
    "    if to in [\"w\", \"win\", \"windows\"]:\n",
    "        path = path.replace(\"/mnt/pa_cpg\", \"Y:\")\n",
    "        path = path.replace(\"/data/pathology\", \"Z:\")\n",
    "        path = path.replace(\"/mnt/pa_cpgarchive1\", \"W:\")\n",
    "        path = path.replace(\"/mnt/pa_cpgarchive2\", \"X:\")\n",
    "        path = path.replace(\"/\", \"\\\\\")\n",
    "    if to in [\"u\", \"unix\", \"l\", \"linux\"]:\n",
    "        path = path.replace(\"Y:\", \"/mnt/pa_cpg\")\n",
    "        path = path.replace(\"Z:\", \"/data/pathology\")\n",
    "        path = path.replace(\"W:\", \"/mnt/pa_cpgarchive1\")\n",
    "        path = path.replace(\"X:\", \"/mnt/pa_cpgarchive2\")\n",
    "        path = path.replace(\"\\\\\", \"/\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def norm_01(x_batch): # Use this for models trained on 0-1 scaled data\n",
    "    x_batch = x_batch / 255\n",
    "    x_batch = x_batch.transpose(3, 0, 1, 2)\n",
    "    return x_batch\n",
    "\n",
    "# def z_norm(x_batch): # use this for default nnunet models, using z-score normalized data\n",
    "#     mean = x_batch.mean(axis=(-2,-1), keepdims=True)\n",
    "#     std = x_batch.std(axis=(-2,-1), keepdims=True)\n",
    "#     x_batch = ((x_batch - mean) / (std + 1e-8))\n",
    "#     x_batch = x_batch.transpose(3, 0, 1, 2)\n",
    "#     return x_batch\n",
    "\n",
    "\n",
    "# def ensemble_softmax_list(trainer, params, x_batch):\n",
    "#     softmax_list = []\n",
    "#     for p in params:\n",
    "#         trainer.load_checkpoint_ram(p, False)\n",
    "#         softmax_list.append(\n",
    "#             trainer.predict_preprocessed_data_return_seg_and_softmax(x_batch.astype(np.float32), verbose=False,\n",
    "#                                                                      do_mirroring=False, mirror_axes=[])[\n",
    "#                 -1].transpose(1, 2, 3, 0).squeeze())\n",
    "#     return softmax_list\n",
    "\n",
    "def ensemble_softmax_list(x_batch):\n",
    "    logits_list = predictor.get_logits_list_from_preprocessed_data(torch.tensor(x_batch, dtype=torch.float32))\n",
    "    softmax_list = [trainer.label_manager.apply_inference_nonlin(logits).numpy() for logits in logits_list]\n",
    "    return softmax_list\n",
    "\n",
    "def array_to_formatted_tensor(array):\n",
    "    # array = np.expand_dims(array.transpose(2, 0, 1), 0)\n",
    "    array = array.transpose(1, 0, 2, 3)\n",
    "    return torch.tensor(array) # need (1, classes, w/h, h/w)\n",
    "\n",
    "# def softmax_list_and_mean_to_uncertainty(softmax_list, softmax_mean):\n",
    "#     loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "#     uncertainty_loss_per_pixel_list = []\n",
    "#     for softmax in softmax_list:\n",
    "#         log_softmax = np.log(softmax + 0.00000001)\n",
    "#         uncertainty_loss_per_pixel = loss(array_to_formatted_tensor(log_softmax),\n",
    "#                                           array_to_formatted_tensor(softmax_mean))\n",
    "#         uncertainty_loss_per_pixel_list.append(uncertainty_loss_per_pixel)\n",
    "#     uncertainty = torch.cat(uncertainty_loss_per_pixel_list).mean(dim=0)\n",
    "#     return uncertainty\n",
    "\n",
    "def softmax_list_and_mean_to_uncertainty(softmax_list, softmax_mean):\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    uncertainty_loss_per_pixel_list = []\n",
    "    for softmax in softmax_list:\n",
    "        log_softmax = np.log(softmax + 0.00000001)\n",
    "        uncertainty_loss_per_pixel = loss(array_to_formatted_tensor(log_softmax),\n",
    "                                          array_to_formatted_tensor(softmax_mean))\n",
    "        uncertainty_loss_per_pixel_list.append(uncertainty_loss_per_pixel)\n",
    "    uncertainty = torch.cat(uncertainty_loss_per_pixel_list).mean(dim=0)\n",
    "    return uncertainty\n",
    "\n",
    "\n",
    "def get_trim_indexes(y_batch):\n",
    "    \"\"\"\n",
    "    Using the y_mask / tissue-background mask we can check if there are\n",
    "    full empty rows and columns with a width of half the model patch size.\n",
    "    We check this in half model patch size increments because otherwise\n",
    "    we screw up the half overlap approach from nnunet (resulting in inconsistent\n",
    "    overlap thoughout the WSI).\n",
    "    We will still need 1 row or column that is empty to make sure the parts that\n",
    "    do have tissue have 4x overlap\n",
    "    \"\"\"\n",
    "    y = y_batch[0]\n",
    "    r_is_empty = [not y[start:end].any() for start, end in zip(half_patch_size_start_idxs, half_patch_size_end_idxs)]\n",
    "    c_is_empty = [not y[:, start:end].any() for start, end in zip(half_patch_size_start_idxs, half_patch_size_end_idxs)]\n",
    "\n",
    "    empty_rs_top = 0\n",
    "    for r in r_is_empty:\n",
    "        if r == True:\n",
    "            empty_rs_top += 1  # count empty rows\n",
    "        else:\n",
    "            trim_top_half_idx = empty_rs_top - 1  # should always include a single empty row, since we need the overlap\n",
    "            trim_top_half_idx = np.clip(trim_top_half_idx, 0, None)  # cannot select regiouns outside sampled patch\n",
    "            trim_top_idx = half_patch_size_start_idxs[trim_top_half_idx]\n",
    "            break\n",
    "\n",
    "    empty_rs_bottom = 0\n",
    "    for r in r_is_empty[::-1]:\n",
    "        if r == True:\n",
    "            empty_rs_bottom += 1\n",
    "        else:\n",
    "            trim_bottom_half_idx = empty_rs_bottom - 1\n",
    "            trim_bottom_half_idx = np.clip(trim_bottom_half_idx, 0, None)\n",
    "            trim_bottom_idx = half_patch_size_end_idxs[::-1][trim_bottom_half_idx]  # reverse index\n",
    "            break\n",
    "\n",
    "    empty_cs_left = 0\n",
    "    for c in c_is_empty:\n",
    "        if c == True:\n",
    "            empty_cs_left += 1\n",
    "        else:\n",
    "            trim_left_half_idx = empty_cs_left - 1\n",
    "            trim_left_half_idx = np.clip(trim_left_half_idx, 0, None)\n",
    "            trim_left_idx = half_patch_size_start_idxs[trim_left_half_idx]\n",
    "            break\n",
    "\n",
    "    empty_cs_right = 0\n",
    "    for c in c_is_empty[::-1]:\n",
    "        if c == True:\n",
    "            empty_cs_right += 1\n",
    "        else:\n",
    "            trim_right_half_idx = empty_cs_right - 1\n",
    "            trim_right_half_idx = np.clip(trim_right_half_idx, 0, None)\n",
    "            trim_right_idx = half_patch_size_end_idxs[::-1][trim_right_half_idx]\n",
    "            break\n",
    "\n",
    "    # print(trim_top_half_idx, trim_bottom_half_idx, trim_left_half_idx, trim_right_half_idx)\n",
    "    # print(trim_top_idx, trim_bottom_idx, trim_left_idx, trim_right_idx)\n",
    "    return trim_top_idx, trim_bottom_idx, trim_left_idx, trim_right_idx\n",
    "\n",
    "\n",
    "def find_matches(img_folder, match_folder, img_extension='', match_extension='', match_contains='', exact_match=False):\n",
    "    img_folder = convert_path(img_folder)\n",
    "    img_files = [item for item in os.listdir(img_folder) if\n",
    "                 os.path.isfile(os.path.join(img_folder, item)) and item.endswith(img_extension)]\n",
    "\n",
    "    match_folder = convert_path(match_folder)\n",
    "    match_files = [item for item in os.listdir(match_folder) if os.path.isfile(os.path.join(match_folder, item))]\n",
    "\n",
    "    # Match and optional filter\n",
    "    if exact_match:\n",
    "        assert match_extension != '', 'exact_match needs a match extension to verify if img stem + match_extension == match filename'\n",
    "        img_match_paths = [(os.path.join(img_folder, img_file),  # add wsi folder in front\n",
    "                            os.path.join(match_folder, match_file))  # add xml folder in front\n",
    "                           for img_file in img_files  # loop over wsi folder files\n",
    "                           for match_file in match_files  # loop over filtered annotaion files\n",
    "                           if match_file.split(convert_path('/'))[-1].startswith(\n",
    "                img_file.split(convert_path('/'))[-1].split('.')[\n",
    "                    0])  # only return if bg file starts with img file name (without suffix)\n",
    "                           and (match_file == img_file.split('.')[0] + match_extension)  # exact match\n",
    "                           and match_contains in match_file.split(convert_path('/'))[\n",
    "                               -1]]  # only return if match contains match_contains\n",
    "\n",
    "    else:\n",
    "        img_match_paths = [(os.path.join(img_folder, img_file),  # add wsi folder in front\n",
    "                            os.path.join(match_folder, match_file))  # add xml folder in front\n",
    "                           for img_file in img_files  # loop over wsi folder files\n",
    "                           for match_file in match_files  # loop over filtered annotaion files\n",
    "                           if match_file.split(convert_path('/'))[-1].startswith(\n",
    "                img_file.split(convert_path('/'))[-1].split('.')[\n",
    "                    0])  # only return if bg file starts with img file name (without suffix)\n",
    "                           and match_contains in match_file.split(convert_path('/'))[\n",
    "                               -1]]  # only return if match contains match_contains\n",
    "\n",
    "    # Checks and prints\n",
    "    if len(img_match_paths) > 0:\n",
    "        matched_img_paths, _ = zip(*img_match_paths)\n",
    "        matched_img_files = [img_path.split(convert_path('/'))[-1] for img_path in matched_img_paths]\n",
    "        unmatched = [img_file for img_file in img_files if img_file not in matched_img_files]\n",
    "        matches = [img_file for img_file in matched_img_files if\n",
    "                   img_file in img_files]  # this captures multiple matches per img_file\n",
    "        if len(unmatched) > 0:\n",
    "            print(f'{len(unmatched)} files were not machted:\\n', unmatched)\n",
    "        else:\n",
    "            print('All image files were matched')\n",
    "        if len(set(matches)) < len(matches):\n",
    "            print('Some matched image files have multiple matches')\n",
    "            raise Exception('Ambiguous')\n",
    "        if len(matches) == len(set(matches)):\n",
    "            print('Each matched image file has a single match')\n",
    "    else:\n",
    "        print('No matches')\n",
    "    print('\\n')\n",
    "    return img_match_paths\n",
    "\n",
    "def make_files_yml_and_return_matches_to_run(matches, files_yml_output_path, output_folder):\n",
    "    \"\"\"\n",
    "    The whole pipeline is costructed in such a way that a <name>_runtime.txt file is created once a python script started to work on this WSI + mask match. This means that this file should not be processed anymore by another python script that is run in parralel (also doesnt need to be copied to the local machine (chis is the case on our computing cluster))\n",
    "    \"\"\"\n",
    "    runtime_stems = [file[:-12] for file in os.listdir(output_folder) if file.endswith('_runtime.txt')]\n",
    "    imgs, _ = zip(*matches)\n",
    "    img_stems = [Path(file).stem for file in imgs]\n",
    "    matches_to_run_idx = [i for i in range(len(img_stems)) if img_stems[i] not in runtime_stems]\n",
    "    matches_to_run = [matches[i] for i in matches_to_run_idx]\n",
    "    \n",
    "    yaml_file = {\"validation\": []}\n",
    "    for wsi, wsa in matches_to_run:\n",
    "        # print(wsi, wsa)\n",
    "        # 1/0\n",
    "        yaml_file[\"validation\"].append({\"wsi\": {\"path\": convert_path(str(wsi), to='linux')},\n",
    "                                        \"wsa\": {\"path\": convert_path(str(wsa), to='linux')}})\n",
    "    with open(convert_path(files_yml_output_path), 'w') as f:\n",
    "        yaml.dump(yaml_file, f)\n",
    "        print('CREATED FILES YAML:', files_yml_output_path)\n",
    "\n",
    "    return matches_to_run\n",
    "\n",
    "def get_closest_spacing(spacing_value):\n",
    "    possible_spacings = [0.25, 0.5, 1, 2, 4, 8, 16]\n",
    "    closest = min(possible_spacings, key=lambda x:abs(x-spacing_value))\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c00379ba-a0e6-40bb-88eb-df5b998c9261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "TEMP WANDB OFF\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "### LOAD MODEL, change your model here\n",
    "#################################################################\n",
    "\n",
    "trainer_results_folder = '/data/pathology/projects/pathology-lung-TIL/nnUNet_v2/data/nnUNet_results/Dataset008_PDL1_simplified_and_union_annotations/nnUNetTrainer_WSD_wei_i0_nnunet_aug__nnUNetWholeSlideDataPlans__wsd_None_iterator_nnunet_aug__2d'\n",
    "plans_dict = load_json(os.path.join(trainer_results_folder, 'plans.json'))\n",
    "dataset_dict = load_json(os.path.join(trainer_results_folder, 'dataset.json'))\n",
    "\n",
    "trainer = nnUNetTrainer_custom_dataloader_test(plans_dict, '2d', 0, dataset_dict) # we need a trainer for a single fold to make use of its inbuilt functions\n",
    "\n",
    "predictor = nnUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=True,\n",
    "    perform_everything_on_gpu=True,\n",
    "    device=torch.device('cuda', 0),\n",
    "    verbose=False,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=False\n",
    ")\n",
    "\n",
    "predictor.initialize_from_trained_model_folder(\n",
    "    trainer_results_folder,\n",
    "    use_folds=(0, 1, 2, 3, 4),\n",
    "    checkpoint_name='checkpoint_best.pth',\n",
    ")\n",
    "\n",
    "# trainer, params = load_model_and_checkpoint_files(model_base_path, folds, mixed_precision=mixed_precision,\n",
    "#                                                   checkpoint_name=checkpoint_name)\n",
    "\n",
    "norm = norm_01 #z_norm if (('score' in trainer_name) or ('default' in trainer_name)) else norm_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdeaf1f-cc4f-4bf7-a962-e6aaa40f29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "### CONFIGS AND PREP, change your settings here\n",
    "#################################################################\n",
    "# check output folder name well, this is where we will store the WSI inference files\n",
    "# output_folder = Path('/data/pathology/projects/pathology-lung-TIL/nnUNet_raw_data_base/inference_results/Task024_TIGER_WSI')\n",
    "output_folder = Path('/data/pathology/projects/pathology-lung-TIL/nnUNet_raw_data_base/inference_results/v2_Task008_PDL1_simplified_and_union_annotations_wei_i0_nnunet_aug_TEST_SET')\n",
    "# output_folder = Path('/data/pathology/projects/pathology-lung-TIL/nnUNet_raw_data_base/inference_results/Task024_TIGER_WSI/ignore0')\n",
    "\n",
    "# f\"/data/pathology/projects/pathology-lung-TIL/nnUNet_raw_data_base/inference_results/{task_name}/WSI_inference/{trainer_name}/{dataset_name}_borderless\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "dataset_name = 'V2_PDL1_test' # name that gets added to folder names and file names\n",
    "\n",
    "# We now make the config and file yamls dynamically, add the file names here\n",
    "files_yml_output_filename = f\"wsi_borderless_inference_files_{dataset_name}.yml\"\n",
    "config_yml_output_filename = f\"wsi_borderless_inference_config_{dataset_name}.yml\"\n",
    "files_yml_output_path = convert_path(rf'Z:\\projects\\pathology-lung-TIL\\pathology-lung-TIL\\code\\config_files\\{files_yml_output_filename}')\n",
    "config_yml_output_path = convert_path(rf'Z:\\projects\\pathology-lung-TIL\\pathology-lung-TIL\\code\\config_files\\{config_yml_output_filename}')\n",
    "\n",
    "\n",
    "### Make files yaml\n",
    "# Enter the folder with the images/wsi and the tissue/bg masks here\n",
    "# Specify the name of the files yaml\n",
    "\n",
    "# TIGER_test_wsi = convert_path(r'Z:\\projects\\tiger\\new_structure\\test-leaderboard-1\\experimental-test-set\\images')\n",
    "# TIGER_test_bg = convert_path(r'Z:\\projects\\tiger\\new_structure\\test-leaderboard-1\\experimental-test-set\\roi-masks')\n",
    "# image_anno_TIGER_test = find_matches(TIGER_test_wsi, TIGER_test_bg)\n",
    "\n",
    "# # TIGER_test_wsi = convert_path(r'Z:\\projects\\tiger\\new_structure\\test-leaderboard-1\\final-test-set\\images')\n",
    "# # TIGER_test_bg = convert_path(r'Z:\\projects\\tiger\\new_structure\\test-leaderboard-1\\final-test-set\\roi-masks')\n",
    "# # image_anno_TIGER_test = find_matches(TIGER_test_wsi, TIGER_test_bg)\n",
    "\n",
    "# matches = image_anno_TIGER_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000025_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000025_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000031_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000031_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000020_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000020_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000001_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000001_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000014_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000014_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000048_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000048_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000011_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000011_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000027_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000027_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000055_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000055_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000024_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000024_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000052_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000052_C0041_L01_A15_roi.tif'), ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000016_C0041_L01_A15.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000016_C0041_L01_A15_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000011_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000011_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000022_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000022_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000024_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000024_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000033_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000033_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000058_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000058_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000060_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000060_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000068_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000068_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000085_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000085_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000091_C0107_B101_V01_T01_L10_A18_E01.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000091_C0107_B101_V01_T01_L10_A18_E01_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0311658.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0311658_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0320537.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0320537_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0506065.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0506065_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0513702.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0513702_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0610332.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0610332_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0708458.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0708458_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/1525458.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/1525458_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/1602737.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/1602737_roi.tif'), ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/1708336.tif', '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/1708336_roi.tif')]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('/data/pathology/projects/pathology-lung-pd-l1/reader_study/inference/cell_annotations/cell_level_cases_dpformat.yaml', 'r') as f:\n",
    "    testset_yaml = yaml.safe_load(f)\n",
    "\n",
    "matches = []\n",
    "for cohort in testset_yaml['data']['cell_level'].keys():\n",
    "    cohort_cases = testset_yaml['data']['cell_level'][cohort]\n",
    "    cohort_image_mask_pairs = [(case['image'], case['mask']) for case in cohort_cases]\n",
    "    matches.extend(cohort_image_mask_pairs)\n",
    "\n",
    "print(len(matches))\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a901e9a-0b18-418e-a846-2467f2ceb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_wsi = convert_path('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000020_C0041_L01_A15.tif')\n",
    "# test_bg = convert_path('/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000020_C0041_L01_A15_roi.tif')\n",
    "# image_anno_test = [(test_wsi, test_bg)]\n",
    "\n",
    "# matches = image_anno_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49fa77f-9f3d-4845-976b-cfb05a67a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000025_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000025_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000031_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000031_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000020_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000020_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000001_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000001_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000014_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000014_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000048_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000048_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000011_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000011_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000027_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000027_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000055_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000055_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000024_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000024_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000052_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000052_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/projects/pathology-lung-pd-l1/reader_study/images/LI_S01_P000016_C0041_L01_A15.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/LI_S01_P000016_C0041_L01_A15_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000011_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000011_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000022_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000022_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000024_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000024_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000033_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000033_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000058_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000058_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000060_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000060_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000068_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000068_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000085_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000085_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pembro_rt/images/PD-L1/IG_S02_P000091_C0107_B101_V01_T01_L10_A18_E01.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/IG_S02_P000091_C0107_B101_V01_T01_L10_A18_E01_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0311658.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0311658_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0320537.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0320537_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0506065.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0506065_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0513702.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0513702_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0610332.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0610332_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/0708458.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/0708458_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/1525458.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/1525458_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/1602737.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/1602737_roi.tif'),\n",
       " ('/data/pathology/archives/lung/pdl1_verona/images/wsi/PD-L1_SP263/1708336.tif',\n",
       "  '/data/pathology/projects/pathology-lung-pd-l1/reader_study/rois/1708336_roi.tif')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED FILES YAML: /data/pathology/projects/pathology-lung-TIL/pathology-lung-TIL/code/config_files/wsi_borderless_inference_files_V2_PDL1_test.yml\n",
      "\n",
      "\n",
      "[DONE] all files are processed already]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "matches = make_files_yml_and_return_matches_to_run(matches, files_yml_output_path, output_folder)\n",
    "if len(matches) == 0:\n",
    "    print('\\n\\n[DONE] all files are processed already]')\n",
    "    sys.exit(0)  # Exit the script successfully\n",
    "###\n",
    "\n",
    "### Make config yaml\n",
    "# Added hints for construction of config file if you want to do it yourself\n",
    "spacing = 0.5 # spacing for batch_shape spacing and annotation_parser output_spacing (leave its processing spacing on 4 or higher)\n",
    "\n",
    "model_patch_size = 512 # input size of model (should be square)\n",
    "half_model_patch_size=int(model_patch_size/2)\n",
    "\n",
    "sampler_patch_size = 3 * model_patch_size\n",
    "assert sampler_patch_size % (model_patch_size/2) == 0 # needed for correct half overlap\n",
    "\n",
    "# due to half overlap there is half the model patch size without overlap on all 4 sides of the sampled patch\n",
    "output_patch_size = sampler_patch_size - 2 * half_model_patch_size # use this as your annotation_parser shape\n",
    "# Note that for this approach we need a CenterPointSampler, and center: True in the patch_sampler and patch_label_sampler\n",
    "\n",
    "tissue_mask_spacing = get_closest_spacing(WholeSlideImage(matches[0][1]).spacings[0])\n",
    "tissue_mask_ratio = tissue_mask_spacing/spacing\n",
    "\n",
    "template = convert_path(r\"Z:\\projects\\pathology-lung-TIL\\pathology-lung-TIL\\code\\config_files\\templates\\borderless_config_v2.yml\")\n",
    "\n",
    "with open(template) as f:\n",
    "    config_yml_str = str(yaml.safe_load(f))\n",
    "\n",
    "replace_dict = {\n",
    "    \"'auto_files_yml'\" : files_yml_output_path,\n",
    "    \"'auto_sampler_patch_size'\" : sampler_patch_size,\n",
    "    \"'auto_spacing'\" : spacing,\n",
    "    \"'auto_tissue_mask_spacing'\" : tissue_mask_spacing,\n",
    "    \"'auto_tissue_mask_ratio'\" : tissue_mask_ratio,\n",
    "    \"'auto_output_patch_size'\" : output_patch_size\n",
    "}\n",
    "\n",
    "print('\\nAuto configuring CONFIG YAML. Replacing template placeholders:')\n",
    "for k, v in replace_dict.items():\n",
    "    print('\\t', k, v)\n",
    "    config_yml_str = config_yml_str.replace(k, str(v))\n",
    "\n",
    "config_yml = yaml.safe_load(config_yml_str)\n",
    "\n",
    "with open(convert_path(config_yml_output_path), 'w') as f:\n",
    "    yaml.dump(config_yml, f)\n",
    "    print('CREATED CONFIG YAML:', config_yml_output_filename)\n",
    "\n",
    "# Some variable settings you can ignore\n",
    "mode = \"validation\"\n",
    "image_path = None\n",
    "previous_file_key = None\n",
    "files_exist_already = None  # not sure if needed\n",
    "plot = False\n",
    "# following is later used to check if we can remove big empty parts of the sampled patch before inference\n",
    "sampler_patch_size_range = list(range(sampler_patch_size))\n",
    "half_patch_size_start_idxs = sampler_patch_size_range[0::half_model_patch_size]\n",
    "half_patch_size_end_idxs = [idx + half_model_patch_size for idx in half_patch_size_start_idxs]\n",
    "\n",
    "wsm_writer = None\n",
    "# wsu_writer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_unfinished = False # this will check if the '<stem>_runtime.txt file is present in the outputfolder. If it is not it means that this file is not processed fully. IMPORTANT: set this to false if you want to run multiple jobs in a parralel way.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed002404-dfe4-4ddf-9f39-4081f3952f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: /data/pathology/projects/pathology-lung-TIL/nnUNet_raw_data_base/inference_results/v2_Task008_PDL1_simplified_and_union_annotations_wei_i0_nnunet_aug_TEST_SET/LI_S01_P000025_C0041_L01_A15_nnunet.tif....\n",
      "Spacing: 0.4850653862456299\n",
      "Dimensions: (135975, 147019)\n",
      "Tile_shape: (1024, 1024)\n",
      "Creating: /data/pathology/projects/pathology-lung-TIL/nnUNet_raw_data_base/inference_results/v2_Task008_PDL1_simplified_and_union_annotations_wei_i0_nnunet_aug_TEST_SET/LI_S01_P000025_C0041_L01_A15_uncertainty.tif....\n",
      "Spacing: 0.4850653862456299\n",
      "Dimensions: (135975, 147019)\n",
      "Tile_shape: (1024, 1024)\n",
      "Total time was 1943\n",
      "Total reading time was 0\n",
      "Total base writing time was 53\n",
      "Total pyramid downsampling time was 0\n",
      "Total pyramid writing time was 1890\n",
      "Total time determining min/max was 7\n",
      "Total time was 1736\n",
      "Total reading time was 0\n",
      "Total base writing time was 70\n",
      "Total pyramid downsampling time was 0\n",
      "Total pyramid writing time was 1666\n",
      "Total time determining min/max was 7\n",
      "[DONE]\n",
      "If there are remaining files that are not being processed right now, set \"rerun_unfinished\" to True right above the ### RUN section\n",
      "\n",
      "\n",
      "\n",
      "[Potential incoming multiprocessing error]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "### RUN\n",
    "#################################################################\n",
    "training_iterator = create_batch_iterator(config_yml_output_path,\n",
    "                                          mode,\n",
    "                                          presets=('slidingwindow.yml',),\n",
    "                                          cpus=4,\n",
    "                                          number_of_batches=-1,\n",
    "                                          return_info=True)\n",
    "\n",
    "for x_batch, y_batch, info in training_iterator:\n",
    "    ### Get image data, check if new image, save previous image if there was one, if new image create new image file\n",
    "    sample_reference = info['sample_references'][0]['reference']\n",
    "    current_file_key = sample_reference.file_key\n",
    "    if current_file_key != previous_file_key:  # if starting a new image\n",
    "        if previous_file_key != None and files_exist_already != True:  # if there was a previous image, and the previous image did not exist already (can also be None)\n",
    "            wsm_writer.save()  # save previous mask\n",
    "            wsu_writer.save()  # save previous uncertainty\n",
    "            # Save runtime\n",
    "            end_time = time.time()\n",
    "            run_time = end_time - start_time\n",
    "            text_file = open(output_folder / (image_path.stem + '_runtime.txt'), \"w\")\n",
    "            text_file.write(str(run_time))\n",
    "            text_file.close()\n",
    "        # Getting file settings and path, doing check if exists already\n",
    "        with training_iterator.dataset.get_wsi_from_reference(sample_reference) as wsi:\n",
    "            image_path = wsi.path\n",
    "            shape = wsi.shapes[wsi.get_level_from_spacing(spacing)]\n",
    "            real_spacing = wsi.get_real_spacing(spacing)\n",
    "        wsm_path = output_folder / (image_path.stem + '_nnunet.tif')\n",
    "        wsu_path = output_folder / (image_path.stem + '_uncertainty.tif')\n",
    "        if rerun_unfinished:\n",
    "            if os.path.isfile(output_folder / (image_path.stem + '_runtime.txt')):\n",
    "                files_exist_already = True  # this means we can skip this whole loop for this file key, checked above '### Prep, predict and uncertainty'\n",
    "                previous_file_key = current_file_key\n",
    "                print(f'[SKIPPING] {image_path.stem} is processed already')\n",
    "                continue  # continue to next batch\n",
    "        elif os.path.isfile(wsm_path) and os.path.isfile(wsu_path):\n",
    "            files_exist_already = True  # this means we can skip this whole loop for this file key, checked above '### Prep, predict and uncertainty'\n",
    "            previous_file_key = current_file_key\n",
    "            print(f'[SKIPPING] {image_path.stem} is processed already or currently being processed')\n",
    "            continue  # continue to next batch\n",
    "        else:\n",
    "            print(f'[RUNNING] {image_path.stem}')\n",
    "            files_exist_already = False\n",
    "        # Create new writer and file\n",
    "        start_time = time.time()\n",
    "        wsm_writer = WholeSlideMaskWriter()  # whole slide mask\n",
    "        wsu_writer = WholeSlideMaskWriter()  # whole slide uncertainty\n",
    "        # Create files\n",
    "        wsm_writer.write(path=wsm_path, spacing=real_spacing, dimensions=shape,\n",
    "                         tile_shape=(output_patch_size, output_patch_size))\n",
    "        wsu_writer.write(path=wsu_path, spacing=real_spacing,\n",
    "                         dimensions=shape, tile_shape=(output_patch_size, output_patch_size))\n",
    "\n",
    "    if files_exist_already:\n",
    "        continue\n",
    "\n",
    "    ### Trim check\n",
    "    trim_top_idx, trim_bottom_idx, trim_left_idx, trim_right_idx = get_trim_indexes(y_batch)\n",
    "    x_batch_maybe_trimmed = x_batch[:, trim_top_idx : trim_bottom_idx, trim_left_idx: trim_right_idx, :]\n",
    "\n",
    "    ### Prep, predict and uncertainty\n",
    "    prep = norm(x_batch_maybe_trimmed)\n",
    "\n",
    "    softmax_list = ensemble_softmax_list(prep)\n",
    "    softmax_mean = np.array(softmax_list).mean(0)\n",
    "    pred_output_maybe_trimmed = softmax_mean.argmax(axis=0)-1\n",
    "\n",
    "    uncertainty = softmax_list_and_mean_to_uncertainty(softmax_list, softmax_mean)\n",
    "    uncertainty_output_maybe_trimmed = np.array((uncertainty.clip(0, 4) / 4 * 255).int()) \n",
    "\n",
    "    ### Reconstruct possible trim\n",
    "    pred_output = np.zeros((sampler_patch_size, sampler_patch_size))\n",
    "    pred_output[trim_top_idx : trim_bottom_idx, trim_left_idx: trim_right_idx] = pred_output_maybe_trimmed\n",
    "\n",
    "    uncertainty_output = np.zeros((sampler_patch_size, sampler_patch_size))\n",
    "    uncertainty_output[trim_top_idx: trim_bottom_idx, trim_left_idx: trim_right_idx] = uncertainty_output_maybe_trimmed\n",
    "\n",
    "    # Only write inner part\n",
    "    pred_output_inner = crop_data(pred_output, [output_patch_size, output_patch_size])\n",
    "    uncertainty_output_inner = crop_data(uncertainty_output, [output_patch_size, output_patch_size])\n",
    "    y_batch_inner = crop_data(y_batch[0], [output_patch_size, output_patch_size]).astype('int64')\n",
    "    \n",
    "    ### Get patch point\n",
    "    point = info['sample_references'][0]['point']\n",
    "    c, r = point[0] - output_patch_size/2, point[1] - output_patch_size/2 # from middle point to upper left point of tile to write\n",
    "    \n",
    "    ### Write tile and set previous file key for next loop check\n",
    "    wsm_writer.write_tile(tile=pred_output_inner * y_batch_inner, coordinates=(int(c), int(r)))\n",
    "    wsu_writer.write_tile(tile=uncertainty_output_inner * y_batch_inner, coordinates=(int(c), int(r)))\n",
    "    previous_file_key = current_file_key\n",
    "\n",
    "if wsm_writer:\n",
    "    wsm_writer.save()  # if done save last image\n",
    "    wsu_writer.save()  # if done save last image\n",
    "\n",
    "    # Save runtime\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    text_file = open(output_folder / (image_path.stem + '_runtime.txt'), \"w\")\n",
    "    text_file.write(str(run_time))\n",
    "    text_file.close()\n",
    "else:\n",
    "    print('\\n\\n[NO FILES TO PROCESS] \\n\\n\\n')\n",
    "\n",
    "training_iterator.stop()\n",
    "print('[DONE]\\nIf there are remaining files that are not being processed right now, set \"rerun_unfinished\" to True right above the ### RUN section')\n",
    "print('\\n\\n\\n[Potential incoming multiprocessing error]\\n\\n')\n",
    "#################################################################\n",
    "### SUBMIT JOB\n",
    "#################################################################\n",
    "# ~/c-submit --require-cpus=8 --require-mem=32g --gpu-count=1 --require-gpu-mem=10G --priority=low 'joepbogaerts' 9974 48 doduo1.umcn.nl/pathology_lung_til/nnunet:9.4-midl2023 python3 /data/pathology/projects/pathology-lung-TIL/pathology-lung-TIL/code/nnUNet_run_WSI_inference_borderless_TIGER.py\n",
    "\n",
    "# ~/c-submit --require-cpus=8 --require-mem=32g --gpu-count=1 --require-gpu-mem=10G --priority=high --constraint=Turing 'joepbogaerts' 9974 48 doduo1.umcn.nl/pathology_lung_til/nnunet:9.4-midl2023 python3 /data/pathology/projects/pathology-lung-TIL/pathology-lung-TIL/code/nnUNet_run_WSI_inference_borderless_TIGER.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wholeslidedata': {'default': {'annotation_parser@replace(true)': {'*object': 'wholeslidedata.annotation.parser.MaskAnnotationParser', 'output_spacing': 0.5, 'processing_spacing': 4.0, 'shape': [1024, 1024]}, 'annotation_parser@replace=true': {'attribute': 'MaskAnnotationParser', 'module': 'wholeslidedata.annotation.parser', 'output_spacing': 0.5, 'processing_spacing': 4.0, 'shape': [1024, 1024]}, 'batch_shape': {'batch_size': 1, 'shape': [1536, 1536, 3], 'spacing': [0.5], 'y_shape': [1536, 1536]}, 'dataset': {'copy_path': '/home/user/data'}, 'image_backend': 'asap', 'patch_label_sampler': {'center': True, 'ratio': 4.0, 'spacing': 2}, 'patch_sampler': {'center': True, 'relative': True}, 'point_sampler_name': 'CenterPointSampler', 'yaml_source': '/data/pathology/projects/pathology-lung-TIL/pathology-lung-TIL/code/config_files/wsi_borderless_inference_files_V2_PDL1_test.yml'}}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(config_yml_output_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
